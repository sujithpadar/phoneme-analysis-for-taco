{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notebook to generate 10,20, 50 Finnish phoneme representation for each English words in the game data.\n",
    "Created from the base: english_word_to_finnish_phoneme.ipynb.\n",
    "\n",
    "Author: Sujith Padaru\n",
    "- *Inputs*: \n",
    "    - Game words\n",
    "    - English Dict, word, phoneme representation form. *[hard coded for en_uk].\n",
    "    - Finnish dict,\n",
    "    - Eng to Global map. Pickle of a dictionary with {phoneme -> phoneme rep}\n",
    "    - Finnish to Global Map.\n",
    "    - Global phone distances.\n",
    "    \n",
    "\n",
    "- *Outputs*: \n",
    "    - n finnish phonetical representation for each word.\n",
    "- *HyperParameters*:\n",
    "    - n number of samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from itertools import product\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Code to Run the below file as a script to generate english words to generate based on different distance \n",
    " metrics and english and finnihs dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_eng_sentence_2_en_uk_phone_rep(sentence):\n",
    "    '''\n",
    "    inputs:\n",
    "    sentence: (str) Sentence which needs to be represented in phones.\n",
    "    outputs:\n",
    "    transcripts (str)['ph ph ph'] Phoneme representation of sentence.\n",
    "    '''\n",
    "    \n",
    "    # Save transcript for each word in the sentence in a list transcripts.\n",
    "    words = sentence.split(' ')    \n",
    "    word_transcripts = []\n",
    "    for word in words:\n",
    "        if word not in en_uk_dict.keys():\n",
    "            return ''\n",
    "        else:\n",
    "            word_transcripts.append(en_uk_dict[word])\n",
    "    \n",
    "    #Take the transcripts in the transcript list and make a phoneme representation.\n",
    "    # Taking care if it's a single word or a sentence.\n",
    "    \n",
    "    sentence_transcript = ''\n",
    "    if len(word_transcripts) == 1:\n",
    "        sentence_transcript += word_transcripts[0]\n",
    "    else:\n",
    "        for i in word_transcripts:\n",
    "            sentence_transcript += i \n",
    "            sentence_transcript += \" sil \"\n",
    "\n",
    "    return sentence_transcript.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_ph_2_global_ph(english_ph_transcript):\n",
    "    '''\n",
    "    input: \n",
    "    english_ph_transcript - English phone transcripts from mapping file. 'ph ph ph ...'\n",
    "    output: \n",
    "    global_ph_transcript: 'ph ph ph ...'\n",
    "    Hyperparameter: eng_to_global_map.\n",
    "    '''\n",
    "    global_ph_transcript = ''\n",
    "    for phone in english_ph_transcript.split(\" \"):\n",
    "        global_ph_transcript += eng_to_global_map[phone]\n",
    "        global_ph_transcript += ' '\n",
    "    \n",
    "    return global_ph_transcript.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_transcript_2_fin_nearest(transcript):\n",
    "    '''Returns the nearest finnish phoneme to each of the global transcript\n",
    "       Input: Global phoneme representation seperated by a space\n",
    "       Output: Finnish phoneme representation seperated by space.\n",
    "    '''\n",
    "    fin_transcript = ''\n",
    "    for phone in transcript.split(\" \"):\n",
    "         \n",
    "        if type(global_2_fin_map[phone]) is list:\n",
    "            fin_transcript += global_2_fin_map[phone][0]\n",
    "        else:\n",
    "            fin_transcript += global_2_fin_map[phone]       \n",
    "        \n",
    "        fin_transcript += ' '\n",
    "    return fin_transcript.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_transcript_2_N_fin(transcript):\n",
    "    '''Returns the nearest finnish phoneme to each of the global transcript\n",
    "       Input: Global phoneme representation seperated by a space\n",
    "       Output: Finnish phoneme representation seperated by space.\n",
    "    '''\n",
    "    \n",
    "    phone_maps = []\n",
    "    for phone in transcript.split(\" \"):\n",
    "        if phone == 'sil':\n",
    "            phone_maps.append(['sil'])\n",
    "        else:\n",
    "            phone_maps.append(global_2_fin_map[phone])\n",
    "    \n",
    "    list_of_trans = list(product(*phone_maps))    \n",
    "    list_of_trans = [\" \".join(list(trans)) for trans in list_of_trans]\n",
    "    return list_of_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_phone_rep(phone_rep):\n",
    "    merged_phone_rep = ''\n",
    "    phones = phone_rep.split(' ')\n",
    "    for phone in phones:    \n",
    "        if phone == 'sil':\n",
    "            merged_phone_rep += ' '\n",
    "        else:\n",
    "            merged_phone_rep += phone\n",
    "    \n",
    "    return merged_phone_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_sentences(sentence):\n",
    "    sentence = sentence.replace(\"  \",\"<sil>\")\n",
    "    sentence = sentence.replace(\" \",\"\")\n",
    "    sentence = sentence.replace(\"<sil>\",\" \")\n",
    "    return sentence\n",
    "\n",
    "def mappings_2_text(index='all',speakers=[600], name='eng_game_words'):\n",
    "    '''\n",
    "    Input : \n",
    "    Index - Index of the the mapping file to be written into a evaluation file.\n",
    "                Default : all the words\n",
    "    speakers - List of speakers the be generated by tacotron.\n",
    "    name - Name of the output text file\n",
    "    \n",
    "    Output:\n",
    "    None, Saves the file in the eval_text/folder. \n",
    "    '''\n",
    "    if index == 'all':\n",
    "        transcript_2_text = mapping[['sentence','fin_transcript']]\n",
    "    else:\n",
    "        transcript_2_text = mapping.iloc[index][['sentence','fin_transcript']]\n",
    "\n",
    "    transcript_2_text.set_index('sentence',inplace=True)\n",
    "    transcript_2_text = transcript_2_text.assign(\n",
    "                                fin_transcript=transcript_2_text.fin_transcript.apply(phone_sentences))\n",
    "    for speaker in speakers:\n",
    "        transcript_2_text = transcript_2_text.assign(speaker_id = len(transcript_2_text)*[speaker])\n",
    "        \n",
    "        my_file = Path('eval_text/{}_{}.txt'.format(name,speaker))\n",
    "        \n",
    "        if my_file.is_file():\n",
    "            print('eval_text/{}_{}.txt already exists; not writing the file'.format(name,speaker))\n",
    "        else:    \n",
    "            transcript_2_text.to_csv('eval_text/{}_{}.txt'.format(name,speaker), sep='|', header=False)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mappings_2_text_N(N=20, index='all', speakers=[600], name='eng_game_words'):\n",
    "    '''\n",
    "    Input : \n",
    "    N : Number of alternate Finnish phonemic representation for each game word.\n",
    "    Index - Index of the the mapping file to be written into a evaluation file.\n",
    "                Default : all the words\n",
    "    speakers - List of speakers the be generated by tacotron.\n",
    "    name - Name of the output text file\n",
    "    \n",
    "    Output:\n",
    "    None, Saves the file in the eval_text/folder. \n",
    "    '''\n",
    "    if index == 'all':\n",
    "        transcript_2_text = mapping[['sentence','all_fin_transcripts']]\n",
    "    else:\n",
    "        transcript_2_text = mapping.iloc[index][['sentence','all_fin_transcripts']]\n",
    "\n",
    "    transcript_2_text.set_index('sentence',inplace=True)\n",
    "    \n",
    "    dfs = []\n",
    "    for sentence, all_fin_transcripts in transcript_2_text.iterrows():\n",
    "        list_of_trans = all_fin_transcripts.values[0]\n",
    "        number_of_trans = len(list_of_trans)\n",
    "\n",
    "        if number_of_trans > 20:\n",
    "            np.random.seed(0)\n",
    "            transcripts = np.random.choice(list_of_trans, size=20, replace=False)\n",
    "            df = pd.DataFrame(transcripts)\n",
    "            df = df.assign(sentence = 20*[sentence])\n",
    "        else:\n",
    "            df = pd.DataFrame(list_of_trans)\n",
    "            df = df.assign(sentence = number_of_trans*[sentence])\n",
    "        dfs.append(df)\n",
    "    \n",
    "    transcript_2_text = pd.concat(dfs)\n",
    "    transcript_2_text.set_index('sentence',inplace=True)\n",
    "    \n",
    "    transcript_2_text= transcript_2_text.assign(merged_transcripts =\n",
    "                                                transcript_2_text[0].apply(merge_phone_rep))\n",
    "    for speaker in speakers:\n",
    "        transcript_2_text = transcript_2_text.assign(speaker_id = len(transcript_2_text)*[speaker])\n",
    "        \n",
    "        my_file = Path('train_synth_text/{}_{}.txt'.format(name,speaker))\n",
    "        \n",
    "        if my_file.is_file():\n",
    "            print('train_synth_text/{}_{}.txt already exists; not writing the file'.format(name,speaker))\n",
    "        else:    \n",
    "            transcript_2_text.to_csv('train_synth_text/{}_spk_{}.txt'.format(name,speaker),\n",
    "                                    columns=[0, 'speaker_id'], sep='|', header=False)\n",
    "            transcript_2_text.to_csv('train_synth_text/merged_{}_spk_{}.txt'.format(name,speaker),\n",
    "                                    columns=['merged_transcripts', 'speaker_id'], sep='|', header=False)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diff_mappings_2_diff_spks_text_N(N=20, index='all', speakers=[600], name='eng_game_words'):\n",
    "    '''\n",
    "    Input : \n",
    "    N : Number of alternate Finnish phonemic representation for each game word.\n",
    "    Index - Index of the the mapping file to be written into a evaluation file.\n",
    "                Default : all the words\n",
    "    speakers - List of speakers the be generated by tacotron.\n",
    "    name - Name of the output text file\n",
    "    \n",
    "    Output:\n",
    "    None, Saves the file in the eval_text/folder. \n",
    "    '''\n",
    "    if index == 'all':\n",
    "        mapping_ = mapping[['sentence','all_fin_transcripts']]\n",
    "    else:\n",
    "        mapping_ = mapping.iloc[index][['sentence','all_fin_transcripts']]\n",
    "\n",
    "    mapping_.set_index('sentence',inplace=True)\n",
    "    \n",
    "    all_transcripts_2_texts = []\n",
    "    for speaker in speakers:\n",
    "        dfs = []\n",
    "        for sentence, all_fin_transcripts in mapping_.iterrows():\n",
    "            list_of_trans = all_fin_transcripts.values[0]\n",
    "            number_of_trans = len(list_of_trans)\n",
    "    \n",
    "            #print('Number of trans=',number_of_trans)\n",
    "            \n",
    "            if number_of_trans > 20:\n",
    "                np.random.seed(speaker)\n",
    "                transcripts = np.random.choice(list_of_trans, size=20, replace=False)\n",
    "                df = pd.DataFrame(transcripts)\n",
    "                df = df.assign(sentence = 20*[sentence])\n",
    "            else:\n",
    "                df = pd.DataFrame(list_of_trans)\n",
    "                df = df.assign(sentence = number_of_trans*[sentence])\n",
    "            dfs.append(df)\n",
    "    \n",
    "        transcript_2_text = pd.concat(dfs)\n",
    "        transcript_2_text.set_index('sentence',inplace=True)\n",
    "        \n",
    "        transcript_2_text= transcript_2_text.assign(merged_transcripts =\n",
    "                                                    transcript_2_text[0].apply(merge_phone_rep))\n",
    "        transcript_2_text = transcript_2_text.assign(speaker_id = len(transcript_2_text)*[speaker])\n",
    "        \n",
    "        all_transcripts_2_texts.append(transcript_2_text)\n",
    "    \n",
    "    df_all_transcripts_2_texts = pd.concat(all_transcripts_2_texts)\n",
    "    #df_all_transcripts_2_texts.to_pickle('df_all_transcripts_2_texts.pickle')\n",
    "    '''\n",
    "        my_file = Path('train_synth_text/{}_{}.txt'.format(name,speaker))\n",
    "        \n",
    "        if my_file.is_file():\n",
    "            print('train_synth_text/{}_{}.txt already exists; not writing the file'.format(name,speaker))\n",
    "        else:    \n",
    "            transcript_2_text.to_csv('train_synth_text_2/{}_spk_{}.txt'.format(name,speaker),\n",
    "                                    columns=[0, 'speaker_id'], sep='|', header=False)\n",
    "            transcript_2_text.to_csv('train_synth_text_2/merged_{}_spk_{}.txt'.format(name,speaker),\n",
    "                                    columns=['merged_transcripts', 'speaker_id'], sep='|', header=False)\n",
    "    '''\n",
    "    return df_all_transcripts_2_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## __main__():\n",
    "Read the Game Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport argparse\\n\\nparser = argparse.ArgumentParser(description=\\'Outputs the Finnish Phoneme representation of the english words\\')\\n\\nparser.add_argument(\"word_list\",help=\"Path to the text file to be converted to Finnish phonetic symbols\",type=str)\\nparser.add_argument(\"-ed\",\"--english_dict\",default=\"dict/en_uk_dict.txt\")\\nparser.add_argument(\\'--eng_to_global_map\\', default=\\'../mappings/en_uk_ph_dist_phones_map.pkl\\',\\n                    help=\"Path to the english to global phoneme dictionary mapping\", type=str)\\nparser.add_argument(\\'--fin_to_global_map\\', default=\\'../mappings/fin_2_global_phones_map.pkl\\',\\n                    help=\"Path to the finnish to global phoneme dictionary mapping\", type=str)\\nparser.add_argument(\\'--global_phone_distances\\', default=\\'../mappings/global_phone_distances.pkl\\',\\n                    help=\"Path to the english to global phoneme dictionary mapping\", type=str)\\nargs = parser.parse_args()\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Outputs the Finnish Phoneme representation of the english words')\n",
    "\n",
    "parser.add_argument(\"word_list\",help=\"Path to the text file to be converted to Finnish phonetic symbols\",type=str)\n",
    "parser.add_argument(\"-ed\",\"--english_dict\",default=\"dict/en_uk_dict.txt\")\n",
    "parser.add_argument('--eng_to_global_map', default='../mappings/en_uk_ph_dist_phones_map.pkl',\n",
    "                    help=\"Path to the english to global phoneme dictionary mapping\", type=str)\n",
    "parser.add_argument('--fin_to_global_map', default='../mappings/fin_2_global_phones_map.pkl',\n",
    "                    help=\"Path to the finnish to global phoneme dictionary mapping\", type=str)\n",
    "parser.add_argument('--global_phone_distances', default='../mappings/global_phone_distances.pkl',\n",
    "                    help=\"Path to the english to global phoneme dictionary mapping\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding the words/sentences to be translated to a **mapping** file, where the further mappings can be representated.\n",
    "mapping = pd.read_csv('eval_text/words.txt',header=None,names=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the English Dictionary \n",
    "- preprocess it\n",
    "- Convert english word rep to eng phoneme rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'English Phoneme Representation samples:'\n",
      "          en_rep\n",
      "word            \n",
      "'cause    kʰ ɒ z\n",
      "'cause    kʰ ə z\n",
      "'dominee     spn\n",
      "'em          ə m\n",
      "'n            n̩\n",
      "'The Dataframe after mapping english words to phonemes:'\n",
      "  sentence     eng_transcript  no_transcript_flag\n",
      "0     girl             g ɜː ɫ               False\n",
      "1    hello           h ɛ l əʊ               False\n",
      "2     book              b ʊ k               False\n",
      "3    learn             l ɜː n               False\n",
      "4  bye bye  b aɪ sil b aɪ sil               False\n"
     ]
    }
   ],
   "source": [
    "# Reading and preprocessing the english dictionary. \n",
    "# English Dictionary should be a text file with each line representeda s follows\n",
    "# language_dialect_word [\\t tab] phone[space]phone[space]...\n",
    "\n",
    "en_uk_dict = pd.read_csv('dict/en_uk_dict.txt',header=None,names=['word','en_rep'],sep='\\t')#,index_col=['word'])\n",
    "\n",
    "#Removing 'language_dialect_' part\n",
    "word_after_remov_en_uk = en_uk_dict.word.apply(lambda en_uk_word: en_uk_word.split('_')[-1])\n",
    "en_uk_dict.set_index(word_after_remov_en_uk,inplace=True)\n",
    "en_uk_dict.drop('word',axis=1,inplace=True)\n",
    "pprint('English Phoneme Representation samples:')\n",
    "pprint(en_uk_dict.head())\n",
    "\n",
    "en_uk_dict = en_uk_dict.to_dict()['en_rep']\n",
    "#Add the English phoneme representation to the mapping dataframe            \n",
    "mapping =mapping.assign(eng_transcript= mapping.sentence.apply(map_eng_sentence_2_en_uk_phone_rep))\n",
    "mapping = mapping.assign(no_transcript_flag=(mapping.eng_transcript==''))\n",
    "pprint('The Dataframe after mapping english words to phonemes:')\n",
    "pprint(mapping.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read English phoneme to Global Phoneme Map\n",
    "- Eng phoneme rep to Global Phoneme rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the English to Global rep and make the mapping in the file.\n",
    "\n",
    "\n",
    "'''Reading the pprint english to global map dictionary\n",
    "{'': '',\n",
    " 'ɒ': 'ɒ',\n",
    " .\n",
    " .\n",
    " .\n",
    " }\n",
    "'''\n",
    "\n",
    "eng_to_global_map = pd.read_pickle('mappings/en_uk_ph_dist_phones_map.pkl')\n",
    "#eng_to_global_map = pd.read_pickle(args.eng_to_global_map)\n",
    "\n",
    "\n",
    "mapping = mapping.assign(global_transcript = mapping.eng_transcript.apply(eng_ph_2_global_ph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Finnish phoneme map to Global Map\n",
    "- Read Global Phone Distance\n",
    "- Infer Global phone to Finnish Phone Map\n",
    "- Find Finnish Phoneme Rep from Global Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentence     eng_transcript  no_transcript_flag  global_transcript  \\\n",
      "0     girl             g ɜː ɫ               False              g ɜ ɫ   \n",
      "1    hello           h ɛ l əʊ               False           h ɛ l əʊ   \n",
      "2     book              b ʊ k               False              b ʊ k   \n",
      "3    learn             l ɜː n               False              l ɜ n   \n",
      "4  bye bye  b aɪ sil b aɪ sil               False  b aɪ sil b aɪ sil   \n",
      "\n",
      "      fin_transcript                                all_fin_transcripts  \n",
      "0              g i l  [g i l, g i lː, g i n, g ɑ l, g ɑ lː, g ɑ n, g...  \n",
      "1            h i l y  [h i l y, h i l ø, h i l øy, h i lː y, h i lː ...  \n",
      "2              b y k  [b y k, b y kː, b y p, b ø k, b ø kː, b ø p, b...  \n",
      "3              l i n  [l i n, l i nː, l i rː, l ɑ n, l ɑ nː, l ɑ rː,...  \n",
      "4  b æe sil b æe sil  [b æe sil b æe sil, b æe sil b eɑ sil, b æe si...  \n"
     ]
    }
   ],
   "source": [
    "# Read the Fin to Global Map and compute Global to Finnish Map.\n",
    "\n",
    "fin_to_global_map = pd.read_pickle('mappings/fin_2_global_phones_map.pkl')\n",
    "global_2_fin_map = dict([[value,key] for key,value in fin_to_global_map.items()])\n",
    "\n",
    "global_phone_dist = pd.read_pickle('mappings/phone_distances.pickle')\n",
    "\n",
    "global_ph = global_phone_dist['phones']\n",
    "global_ph_dist = global_phone_dist['phone_distances']\n",
    "\n",
    "global_ph_dist = pd.DataFrame(data=global_ph_dist,index=global_ph.values(),columns=global_ph.values())\n",
    "\n",
    "distance_2_fin = global_ph_dist.loc[global_2_fin_map.keys()]\n",
    "\n",
    "for ph in global_ph.values():\n",
    "    three_nearest_phones = list(distance_2_fin[ph].sort_values()[:3].index)\n",
    "    global_2_fin_map[ph] = three_nearest_phones\n",
    "\n",
    "\n",
    "global_2_fin_map['w'] = np.concatenate([global_2_fin_map['w'][:2],['v']])\n",
    "global_2_fin_map['w'] = list(global_2_fin_map['w'])\n",
    "global_2_fin_map['z'] = np.concatenate([global_2_fin_map['z'][:2],['s']])\n",
    "global_2_fin_map['z'] = list(global_2_fin_map['z'])\n",
    "\n",
    "mapping = mapping.assign(fin_transcript=mapping.global_transcript.apply(glob_transcript_2_fin_nearest))\n",
    "\n",
    "\n",
    "mapping = mapping.assign(all_fin_transcripts =\n",
    "                         mapping.global_transcript.apply(glob_transcript_2_N_fin))\n",
    "pprint(mapping.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the Dataframe to save the evaluation file.\n",
    "- Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating 20 fin reps for all random words with speaker, 600, 555, 567 and 57, 258 \n",
    "\n",
    "rand_spks = range(551,601)\n",
    "mappings_2_text_N(speakers=rand_spks, name='all_words_20_rep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 20 fin reps for all random words with speaker, 600, 555, 567 and 57, 258 \n",
    "\n",
    "rand_spks = range(551,601)\n",
    "#mappings_2_text_N(speakers=rand_spks, name='all_words_20_rep')\n",
    "df = diff_mappings_2_diff_spks_text_N(speakers=rand_spks, name='all_words_20_random_rep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_to_unmerged = df[[0,'merged_transcripts']]\n",
    "merged_to_unmerged.set_index('merged_transcripts',inplace=True,drop=True)\n",
    "merged_to_unmerged.columns = ['unmerged_phones']\n",
    "merged_to_unmerged.to_pickle('merged_to_unmerged_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating 20 fin reps for 5 random words with speaker, 600, 555, 567 and 57, 258 \n",
    "\n",
    "indexes = mapping.sample(5).index\n",
    "rand_spks = [600, 555, 567, 57, 258]\n",
    "mappings_2_text_N(index=indexes, speakers=rand_spks, name='5_words_20_rep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
